---
title: "Simple Linear Regression"
author: "Heaven Klair"
date: "1/17/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I will use the template that I made in the pre-processing part of the project

## Importing the Dataset
```{r, echo = TRUE}
dataset = read.csv('Salary_Data.csv')
print(head(dataset))

# install.packages('caTools')
library(caTools)
set.seed(123)

# The dataset contains 30 observations, so let's take 20 observations in the
# training set and and 10 observations in the test set. Split Ratio = 2/3
split = sample.split(dataset$Salary, SplitRatio = 2/3)


training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

The simple linear regression package in R already takes care of the feature scaling, so we don't need feature scaling here.

## Fitting Simple Linear Regression to the Training Set
```{r, echo = TRUE}
regressor = lm(formula = Salary ~ YearsExperience,
               data = training_set)

print(summary(regressor))
```
## Interpreting the results from the summary function

Residuals section contain the summary of the residuals (the distance from the data to the fitted line). Ideally, they should be symmetrically distributed around the line; that means the Min value and Max value to be approximately the same distance from 0. Likewise, we want the first Quantile (1Q) and the third Quantile (3Q) to be equidistant from 0. Also, it is nice to have median close to 0.

The next section of Coefficients tells us about least squares estimates for the fitted line.

1. The Intercept tells us about the value of the intercept and independent variable is the slope in the equation of a straight line.

2. Std Error of the estimates and the T-value shows how p-values were calculated. These p-values test whether the estimates for the intercept and the slope are equal to 0 or not. If not, they don;t have much use in the model.

3. Pr>(|t|): These are the p-values for the estimated parameters. We know that lower the P-value is, the more significant the independent variable is going to be. This means the more impact, the more effect the independent variable is going to have on the dependent variable. Ideal number for P-value is less than 5%. So, we want the p-value for a independent variable to be less than 0.05.

4. Residual standard error: This is the square root of the denominator in the equation for F.

5. Multiple R-squared: It is just $R^2$. In this example, it means "Years of Experience can explain 96% of the variation in the salary."

6. Adjusted R-squared: It is the $R^2$ scaled by the number of parameters in the model.

7. F-statistic: It is the value for F along with Degree of Freedom(DF)

8. p-value: Here is the p-value.

## Predicting the Test Set Results
```{r, echo = TRUE}
y_pred = predict(regressor, newdata = test_set)
print(y_pred)
```

## Visualising the Training set results
```{r, echo = TRUE}
# install.packages('ggplot2')
library(ggplot2)

ggplot() + 
  geom_point(aes(x=training_set$YearsExperience, y = training_set$Salary),
             color = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            color = 'blue') + 
  ggtitle('Salary vs Experience (Training Set)') +
  xlab('Years of Expereince ')+
  ylab('Salary')

```

This geom_point function will plot the scatter plot and the geom_line function will plot all the predicted salaries of the training set observations



## Visualising the Test set results
```{r, echo = TRUE}
ggplot() + 
  geom_point(aes(x=test_set$YearsExperience, y = test_set$Salary),
             color = 'red') + 
  geom_line(aes(x = training_set$YearsExperience, y = predict(regressor, newdata = training_set)),
            color = 'blue') + 
  ggtitle('Salary vs Experience (Training Set)') +
  xlab('Years of Expereince ')+
  ylab('Salary')

```




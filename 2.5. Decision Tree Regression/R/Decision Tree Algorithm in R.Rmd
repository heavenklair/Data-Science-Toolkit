---
title: "Decision Tree Algorithm in R"
author: "Heaven Klair"
date: "4/12/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Decision Tree Algorithm in R

## Importing the dataset
```{r, echo=TRUE}
dataset = read.csv('Position_Salaries.csv')
dataset = dataset[2:3]
```

## Splitting the dataset into the Training and Test set
There is no need to split the dataset into training set and test set for the Position_Salaries csv file because the dataset is pretty small, and we want to get as much values as we can for the training purpose. 

If the dataset is large enough, below is the template for splitting the dataset into training and test set.

```{r, echo=TRUE}
# install.packages('caTools')
# library(caTools)
# set.seed(123)
# split = sample.split(dataset$DependentVariable, SplitRatio = 0.8)
# training_set = subset(dataset, split == TRUE)
# test_set = subset(dataset, split == FALSE)
```


## Feature Scaling
Since, we did not create a Training or Test set, we do not need to do feature scaling. However a template to feature scale the dataset is below
```{r, echo=TRUE}
#training_set = scale(training_set)
#test_set = scale(test_set)
```

## Fitting Regression Model to the Dataset
```{r, echo=TRUE}
#install.packages("rpart")
library(rpart)
regressor = rpart(formula = Salary ~.,
                  data = dataset)

```

## Predicting a new result with Decision Tree Regression Model
```{r, echo=TRUE}
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred
```
We can see that the value of the Salary at a 6.5 level is much higher than predicted by the other models. 


## Visualising the Decision Tree Regression Results
```{r, echo=TRUE}
library(ggplot2)
ggplot() + 
  geom_point(aes(x = dataset$Level, y = dataset$Salary),
             colour = 'red') +
  geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset )),
             colour = 'blue') +
  ggtitle('Truth or Bluff (Decision Tree Regression)') +
  xlab('Level') +
  ylab('Salary')
```
We encountered a similar situation with the similar graph in the SVR Module for Python. The reason that we got this kind of graph there was we didn't apply feature scaling to the dataset. What is the problem here? Same (we didn't apply feature scaling).

No, it is not because of the fact that we did not apply feature scaling because we know we do not need to apply feature scaling while implementing decision tree algorithm because decision trees are based on conditions on the independent variables that has no nothing to do with euclidean distances. We only apply feature scaling when the machine learning model is based on euclidean distances, and the scale of dependent and independent variable is too much.

This is not the problem here. So we can rule out feature scaling.

The single line the graph represents the split, and since there is only one line, we can conclude that the decision tree model is making a single split at Salary = 249500, and which is the average salary of the dataset. So the decision tree is just outputting the average value of the dataset. This is not what we want to achieve.

In this case, we will be using a parameter from the rpart library to correct our decision tree model.

## Fitting Regression Model Again with the additional parameters
```{r, echo=TRUE}

regressor = rpart(formula = Salary ~.,
                  data = dataset,
                  control = rpart.control(minsplit = 1) )
```


## Predicting a new result with New Model
```{r, echo=TRUE}
y_pred = predict(regressor, data.frame(Level = 6.5))
y_pred
```

## Visualising the Decision Tree Regression Results
```{r, echo=TRUE}
library(ggplot2)
ggplot() + 
  geom_point(aes(x = dataset$Level, y = dataset$Salary),
             colour = 'red') +
  geom_line(aes(x = dataset$Level, y = predict(regressor, newdata = dataset )),
             colour = 'blue') +
  ggtitle('Truth or Bluff (Decision Tree Regression)') +
  xlab('Level') +
  ylab('Salary')
```

Now, we see something different. There are more number of splits that the first plot, but is this a correct plot? What does the intuition say?


## Visualising the Decision Tree Regression Model Results (Higher Resolution)
```{r, echo=TRUE}
library(ggplot2)
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)

ggplot() + 
  geom_point(aes(x = dataset$Level, y = dataset$Salary),
             colour = 'red') +
  geom_line( aes( x = x_grid, y = predict(regressor, newdata = data.frame( Level = x_grid ))),
             colour = 'blue') +
  ggtitle('Decision Tree Regression Model') +
  xlab('X Label') +
  ylab('Y Label')
```

Looking at the plot above, we can interpret it by:
The horizontal straight lines represents the line which is splitting the data points. We can what are those intervals in which the data is being split. The first interval is from 0 to approximately 6.5 and the second being from that value to 8.5. The decision tree in this case will consider the average of the interval and assign that as the label of that interval. The decision tree in 1D is not a very good model, but it can be very interesting in more dimensions.